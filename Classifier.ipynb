{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier Example\n",
    "The task is to predict whether the GDP per capita for a country is more than the average GDP, based on the following features:<br>\n",
    "\n",
    "-  Population density (per suqare km)<br>\n",
    "-  Population growth rate (%)<br>\n",
    "-  Urban population (%)<br>\n",
    "-  Life expectancy at birth (years)<br>\n",
    "-  Fertility rate (births per woman)<br>\n",
    "-  Infant mortality (deaths per 1000 births)<br>\n",
    "-  Enrolment in tertiary education (%)<br>\n",
    "-  Unemployment (%)<br>\n",
    "-  Estimated control of corruption (score)<br>\n",
    "-  Estimated government effectiveness (score)<br>\n",
    "-  Internet users (per 100 people)<br>\n",
    "\n",
    "120 examples are provided for training and 40 for testing. Each row represents one country, the first column is the label, followed by the features. The feature values have been normalised, by subtracting the mean and dividing by the standard deviation. The label is 1 if the GDP is more than average, and 0 otherwise.\n",
    "\n",
    "\n",
    "Based on----> https://github.com/marekrei/<br>\n",
    "Based on----> https://github.com/marekrei/<br>\n",
    "Based on----> https://github.com/marekrei/<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import theano as T \n",
    "import numpy as np\n",
    "import sys \n",
    "import collections\n",
    "floatX = T.config.floatX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(object):\n",
    "    def __init__ (self,inputSize,hidden_layers,lambda_regularization):\n",
    "        random_seed=42 #Ensure reproducibility\n",
    "        hiddenSize=hidden_layers\n",
    "        lambdaL2=lambda_regularization\n",
    "        \n",
    "        rng=np.random.RandomState(random_seed)\n",
    "        \n",
    "        \n",
    "        #Variables for the squeleton\n",
    "        myInput=T.tensor.fvector('input_vector')\n",
    "        target=T.tensor.fscalar('target')\n",
    "        learningRate=T.tensor.fscalar('learningRate')\n",
    "        \n",
    "        W_inputs_hidden_vals = np.asarray(rng.normal(loc=0.0,scale=0.1,size=(inputSize,hiddenSize)),dtype=floatX)\n",
    "        W_inputs_hidden = T.tensor.shared(W_inputs_hidden_vals,'W_inputs_hidden')\n",
    "        \n",
    "        \n",
    "        hidden=T.tensor.dot(myInput,W_inputs_hidden)\n",
    "        hidden=T.tensor.nnet.sigmoid(hidden)\n",
    "        \n",
    "        W_hidden_output_vals=np.asarray(rng.normal(loc=0.0,scale=0.1,size=(inputSize,hiddenSize)),dtype=floatX)\n",
    "        W_hidden_output = T.tensor.shared(W_hidden_output_vals,'W_hidden_output')\n",
    "        \n",
    "        output=T.tensor.dot(hidden,W_hidden_output)\n",
    "        output=T.tensor.nnet.sigmoid(output)\n",
    "        \n",
    "        cost=T.tensor.sqr(output-target)\n",
    "        cost += lambdaL2 * (T.tensor.sqr(W_hidden_output).sum() + T.tensor.sqr(W_inputs_hidden).sum())\n",
    "        \n",
    "        \n",
    "        params = [W_inputs_hidden,W_hidden_output]\n",
    "        gradients = T.tensor.grad(cost,params)\n",
    "        #W_updated=W-(0.01*gradients[0])\n",
    "        #updates=[(W,W_updated)]\n",
    "        #These two lines above are the same as the one below. Updates is a list of tuples.\n",
    "        updates = [(param,param-(learningRate * gra)) for param,grad in zip(params,gradients)]\n",
    "        \n",
    "        \n",
    "        #####################################################################################################\n",
    "        ##########################################                                           ################\n",
    "        ##########################################   After the skeleton define the functions ################\n",
    "        ##########################################                                           ################\n",
    "        #####################################################################################################\n",
    "        \n",
    "        self.train = T.function([myInput,target,learningRate],[cost,output],updates=updates, allow_input_downcast=True)\n",
    "        self.test = T.function([myInput,target],[cost,output],allow_input_downcast=True)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(path):\n",
    "    #Each line of the dataset is an example, which is labeled (fisrt col) and \n",
    "    #is followed by the different features\n",
    "    \n",
    "    dataset=[]\n",
    "    with open(path,\"r\")as f:\n",
    "        for line in f:\n",
    "            #for each line, we get the label and features\n",
    "            line_parts = line.strip().split()\n",
    "            label = float(line_parts[0])\n",
    "            vector= np.array([float(line_parts[i]) for i in range(1,len(line_parts))])\n",
    "            dataset.append(label,vector)\n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #If we are going to call this from the console this way:\n",
    "    #python classifier.py data/countries-classify-gdp-normalised.train.txt data/countries-classify-gdp-normalised.test.txt\n",
    "    #path_train = sys.argv[1]\n",
    "    #path_test = sys.argv[2]\n",
    "    \n",
    "    learningRate=0.1\n",
    "    epochs = 10 #10 runs over the whole dataset\n",
    "    \n",
    "    data_train = read_dataset()\n",
    "    data_test = read_dataset()\n",
    "    \n",
    "    numberFeatures = len(data_train[0][1])\n",
    "    myClassifier = Classifier(inputSize=numberFeatures,hidden_layers=5,lambda_regularization=0.001)\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        costSum=0.0\n",
    "        correct=0\n",
    "        for label,vector in data_train:\n",
    "            cost,predicted_value = myClassifier.train(vector,label,learningRate)\n",
    "            costSum += cost\n",
    "            if (label == 1.0 and predicted_value >= 0.5) or (label == 0.0 and predicted_value < 0.5):\n",
    "                 correct += 1\n",
    "        print(\"Epoch: \" + str(epoch) + \", Training_cost: \" + str(cost_sum) + \", Training_accuracy: \" + str(float(correct) / len(data_train)))\n",
    "        \n",
    "    #Testing\n",
    "    \n",
    "    cost_sum = 0.0 \n",
    "    correct = 0\n",
    "    for label,vector in data_test:\n",
    "        cost, predicted_value = classifier.test(vector, label)\n",
    "        cost_sum += cost\n",
    "        if (label == 1.0 and predicted_value >= 0.5) or (label == 0.0 and predicted_value < 0.5):\n",
    "            correct += 1\n",
    "    print ( \"Test_cost: \" + str(cost_sum) + \", Test_accuracy: \" + str(float(correct) / len(data_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
